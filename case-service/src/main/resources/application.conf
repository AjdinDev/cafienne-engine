//Default application.conf runs the service based on local files out of the box.
//This is not a production setup. Please have a look in the run/case-service folder for
//a better setup with cassandra as event store and Postgres as view store.
projectionsDB {
  profile = "slick.jdbc.H2Profile$"
  db {
    connectionPool = disabled
    keepAliveConnection = true
    driver = "org.h2.Driver"
    url = "jdbc:h2:mem:tsql1;DB_CLOSE_DELAY=-1"
    migrateUser = ""
    migratePwd = ""
  }
  debug = false
}

akka {
  loglevel = INFO
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 10s

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    serialize-messages = on

    serializers {
      command_serializer = "org.cafienne.akka.actor.serialization.CommandSerializer"
      response_serializer = "org.cafienne.akka.actor.serialization.ResponseSerializer"
      event_serializer = "org.cafienne.akka.actor.serialization.EventSerializer"
      // offset_serializer is used to serialize offset snapshots
      offset_serializer = "org.cafienne.infrastructure.eventstore.OffsetSerializer"
    }
    serialization-bindings {
      "org.cafienne.akka.actor.event.ModelEvent" = event_serializer
      "org.cafienne.akka.actor.command.ModelCommand" = command_serializer
      "org.cafienne.akka.actor.command.response.ModelResponse" = response_serializer
      // Current offsets are WrappedOffset objects
      "org.cafienne.infrastructure.eventstore.WrappedOffset" = offset_serializer
      // enable below to check if all events have been serialized without java.io.Serializable
      //"java.io.Serializable" = none
    }

    provider = "akka.cluster.ClusterActorRefProvider"
  }

  remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    seed-nodes = ["akka.tcp://ClusterSystem@127.0.0.1:2552"]

    auto-down-unreachable-after = 10s
  }

  persistence {
    journal {
      # DO NOT USE LEVELDB FOR A MULTI NODE SETUP !!!
      plugin = "akka.persistence.journal.leveldb"

      leveldb {
        store {
          # DO NOT USE 'native = off' IN PRODUCTION !!!
          native = off
          dir = "journal"
        }

        event-adapters {
          tagging = "org.cafienne.akka.actor.tagging.CaseTaggingEventAdapter"
        }

        event-adapter-bindings {
          "org.cafienne.akka.actor.event.ModelEvent" = tagging
        }
      }
    }
    snapshot-store {
      # Path to the snapshot store plugin to be used
      plugin = "akka.persistence.snapshot-store.local"

      # Local filesystem snapshot store plugin.
      local {
        # Class name of the plugin.
        class = "akka.persistence.snapshot.local.LocalSnapshotStore"
        # Dispatcher for the plugin actor.
        plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
        # Dispatcher for streaming snapshot IO.
        stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
        # Storage location of snapshot files.
        dir = "snapshots"
      }
    }
  }

  contrib.cluster.sharding {
    # The extension creates a top level actor with this name in top level user scope,
    # e.g. '/user/sharding'
    guardian-name = sharding
    # If the coordinator can't store state changes it will be stopped
    # and started again after this duration.
    coordinator-failure-backoff = 10 s
    # Start the coordinator singleton manager on members tagged with this role.
    # All members are used if undefined or empty.
    # ShardRegion actor is started in proxy only mode on nodes that are not tagged
    # with this role.

    #role = "case-domain"
    #role = "domain"

    # The ShardRegion retries registration and shard location requests to the
    # ShardCoordinator with this interval if it does not reply.
    retry-interval = 2 s
    # Maximum number of messages that are buffered by a ShardRegion actor.
    buffer-size = 100000
    # Timeout of the shard rebalancing process.
    handoff-timeout = 60 s
    # Rebalance check is performed periodically with this interval.
    rebalance-interval = 10 s
    # How often the coordinator saves persistent snapshots, which are
    # used to reduce recovery times
    snapshot-interval = 3600 s
    # Setting for the default shard allocation strategy
    least-shard-allocation-strategy {
      # Threshold of how large the difference between most and least number of
      # allocated shards must be to begin the rebalancing.
      rebalance-threshold = 10
      # The number of ongoing rebalancing processes is limited to this number.
      max-simultaneous-rebalance = 3
    }
  }

}

cafienne {
  # Platform has owners that are allowed to create/disable/enable tenants
  #  This property specifies the set of user-id's that are owners
  #  This array may not be empty.
  platform {
    owners = ["admin"]
  }

  api {
    bindhost = "localhost"
    bindport = 2027

    security {
      # configuration settings for OpenID Connect
      oidc {
        connect-url = "http://localhost:5556/dex/.well-known/openid-configuration"
        token-url = "http://127.0.0.1:5556/dex/token"
        key-url = "http://127.0.0.1:5556/dex/keys"
        authorization-url = "http://127.0.0.1:5556/dex/auth"
        issuer = "http://localhost:28080/dex"
      }

      # Fill this setting to true to allow developers to access
      # engine events without authentication
      debug.events.open = false
    }
  }

  # The case engine reads definitions as XML files from disk and/or the classpath.
  # The files are cached in-memory, based on their lastModified timestamp
  # (i.e., if you change a file on disk, the engine will reload it into the cache).
  # By default, the engine will read from the configured location. If the definitions file cannot be found
  # in this location, the engine will try to load it as a resource from the classpath, hence enabling to ship
  # fixed definitions in a jar file.
  definitions {
    provider = "org.cafienne.cmmn.repository.file.FileBasedDefinitionProvider"
    location = "./definitions"
    cache {
      size = 100
    }
  }

  actor {
    # the seconds of idle time after which a case actor is removed from akka memory
    # if the case has not received new commands after the specified number of seconds,
    # the case engine will ask akka to remove the case from memory to avoid memory leaks.
    idle-period = 600
  }

  elastic-connection {
    # Settings for trying to reconnect to Elastic Search upon starting the case-service
    retry {
      # Number of attempts we should try to connect to ElasticSearch; defaults to 6
      attempts = 6
      # Interval in milliseconds to wait inbetween connection attempts; defaults to 5000
      interval = 5000
    }
    host-list = "elasticsearch://localhost:9300"
  }

  elasticsearch {
    cluster.name = "elastic-cafienne"
    #client.transport.sniff = true
    # sample setup for use with Amazon WS
    #discovery.zen.ping.multicast.enabled = true
    #discovery.zen.ping.multicast.ping.enabled = false
    #discovery.zen.ping.unicast.hosts = ["80.79.203.63:9300"]
  }

}
