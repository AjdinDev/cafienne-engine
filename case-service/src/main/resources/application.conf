# Default application.conf runs the Case Service with
#  Cassandra as storage for events and
#  Postgres as storage for the projections to enable the Case queries and
#  Akka as the runtime system
# All of these can be configured through this conf file.
# In addition many settings can also be passed as environment variables

akka {
  loglevel = INFO
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 10s

  actor {
    serialize-messages = on

    serializers {
      command_serializer = "org.cafienne.akka.actor.serialization.CommandSerializer"
      response_serializer = "org.cafienne.akka.actor.serialization.ResponseSerializer"
      event_serializer = "org.cafienne.akka.actor.serialization.EventSerializer"
      # offset_serializer is used to serialize offset snapshots
      offset_serializer = "org.cafienne.infrastructure.eventstore.OffsetSerializer"
    }
    serialization-bindings {
      "org.cafienne.akka.actor.event.ModelEvent" = event_serializer
      "org.cafienne.akka.actor.command.ModelCommand" = command_serializer
      "org.cafienne.akka.actor.command.response.ModelResponse" = response_serializer
      # Current offsets are WrappedOffset objects
      "org.cafienne.infrastructure.eventstore.WrappedOffset" = offset_serializer
      # enable below to check if all events have been serialized without java.io.Serializable
      #"java.io.Serializable" = none
    }
  }

  persistence {
    journal {
      #plugin = "cassandra-journal"
      plugin = "jdbc-journal"

      auto-start-journals = ["jdbc-journal"]
    }
    snapshot-store {
      # default snapshot store is in jdbc

      #plugin = cassandra-snapshot-store
      plugin = "jdbc-snapshot-store"
      # Enable the line below to automatically start the snapshot-store when the actorsystem is started
      auto-start-snapshot-stores = ["jdbc-snapshot-store"]
    }
  }
}

cafienne {
  # Platform has owners that are allowed to create/disable/enable tenants
  #  This property specifies the set of user-id's that are owners
  #  This array may not be empty.
  platform {
    owners = ["admin"]
    owners = ${?CAFIENNE_PLATFORM_OWNERS}
    default-tenant = "world"
    default-tenant = ${?CAFIENNE_PLATFORM_DEFAULT_TENANT}
  }

  debug = false

  api {
    bindhost = "localhost"
    bindport = 2027

    security {
      # configuration settings for OpenID Connect
      oidc {
        connect-url = "http://localhost:5556/dex/.well-known/openid-configuration"
        connect-url = ${?CAFIENNE_OIDC_CONNECT_URL}
        token-url = "http://127.0.0.1:5556/dex/token"
        token-url = ${?CAFIENNE_OIDC_TOKEN_URL}
        key-url = "http://127.0.0.1:5556/dex/keys"
        key-url = ${?CAFIENNE_OIDC_KEY_URL}
        authorization-url = "http://127.0.0.1:5556/dex/auth"
        authorization-url = ${?CAFIENNE_OIDC_AUTHORIZATION_URL}
        issuer = "http://localhost:28080/dex"
        issuer = ${?CAFIENNE_OIDC_ISSUER}

        # Below settings can be used to configure to the untrustworthy token service
        #  Only use this to run the Cafienne Testscript Framework for test purposes.
        #  Never use this in production systems.
        #connect-url = "http://localhost:2377/.well-known/openid-configuration"
        #token-url = "http://localhost:2377/token"
        #key-url = "http://localhost:2377/keys"
        #authorization-url = "http://localhost:2377/auth"
        #issuer = "Cafienne Test Framework"
      }

      # Fill this setting to true to allow developers to access
      # engine events without authentication
      debug.events.open = false
      debug.events.open = ${?CAFIENNE_DEBUG_EVENTS}
    }
  }

  # The case engine reads definitions as XML files from disk and/or the classpath.
  # The files are cached in-memory, based on their lastModified timestamp
  # (i.e., if you change a file on disk, the engine will reload it into the cache).
  # By default, the engine will read from the configured location. If the definitions file cannot be found
  # in this location, the engine will try to load it as a resource from the classpath, hence enabling to ship
  # fixed definitions in a jar file.
  definitions {
    provider = "org.cafienne.cmmn.repository.file.FileBasedDefinitionProvider"
    location = "./definitions"
    location =  ${?CAFIENNE_CMMN_DEFINITIONS_PATH}
    cache {
      size = 100
    }
  }

  actor {
    # the seconds of idle time after which a case actor is removed from akka memory
    # if the case has not received new commands after the specified number of seconds,
    # the case engine will ask akka to remove the case from memory to avoid memory leaks.
    idle-period = 600
  }

  query-db {
    # This setting tells cafienne which journal to use for reading events.
    #  If this omitted, cafienne will try to guess the read journal, based on the akka settings
    read-journal = "jdbc-read-journal"
    #read-journal = "cassandra-query-journal"

    profile = "slick.jdbc.PostgresProfile$"
    profile = ${?PROJECTION_DB_PROFILE}
    db {
      driver = "org.postgresql.Driver"
      driver =  ${?PROJECTION_DB_DRIVER}
      url = "jdbc:postgresql://localhost:5432/cafienne-query?reWriteBatchedInserts=true"
      url =  ${?PROJECTION_DB_URL}
      // User name to connect, update and query
      user = "postgres"
      user =  ${?PROJECTION_DB_USER}
      password = "postgres"
      password =  ${?PROJECTION_DB_PASSWORD}
      numThreads = 10
      connectionTimeout = 5000
      validationTimeout = 5000
    }
  }
}

#################################################################################
#                                                                               #
#  Below are settings for Akka Event Storage for both PostgreSQL and Cassandra  #
#                                                                               #
#################################################################################


# Config settings for using JDBC persistence for storing of Akka Events (and snapshots)
#  Default to a PostgreSQL database instance with name 'cafienne-eventstore'
akka-persistence-jdbc {
  database-provider-fqcn = "org.cafienne.service.db.events.EventDatabaseProvider"
  shared-databases {
    slick {
      profile ="slick.jdbc.PostgresProfile$"
      profile = ${?EVENT_DB_PROFILE}
      db {
        connectionPool = disabled
        driver = "org.postgresql.Driver"
        driver = ${?EVENT_DB_DRIVER}
        url = "jdbc:postgresql://localhost:5432/cafienne-eventstore?reWriteBatchedInserts=true"
        url = ${?EVENT_DB_URL}
        // User name to connect, update and query
        user = ""
        user = ${?EVENT_DB_USER}
        password = ""
        password = ${?EVENT_DB_PASSWORD}
        numThreads = 5
        connectionTimeout = 5000
        validationTimeout = 5000
        maxConnections = 5
        minConnections = 1
      }
    }
  }
}

jdbc-journal {
  use-shared-db = "slick"

  event-adapters {
    tagging = "org.cafienne.akka.actor.tagging.CaseTaggingEventAdapter"
  }

  event-adapter-bindings {
    "org.cafienne.akka.actor.event.ModelEvent" = tagging
  }
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
  use-shared-db = "slick"
}

# the akka-persistence-query provider in use
jdbc-read-journal {
  use-shared-db = "slick"
}

# Config settings for Cassandra as Event store
cassandra-journal {
  keyspace-autocreate = true
  tables-autocreate = true

  event-adapters {
    tagging = "org.cafienne.akka.actor.tagging.CaseTaggingEventAdapter"
  }

  event-adapter-bindings {
    "org.cafienne.akka.actor.event.ModelEvent" = tagging
  }

  events-by-tag {
    query-plugin = cassandra-query-journal
  }
}

cassandra-query-journal {
  # Implementation class of the Cassandra ReadJournalProvider
  class = "akka.persistence.cassandra.query.CassandraReadJournalProvider"

  # First time bucket dramatically improves startup performance if the offset for projections starts with NoOffset
  first-time-bucket = "20200101T00:00"

  session-provider = "akka.cassandra.session.DefaultSessionProvider"
  session-name = ""

  # Absolute path to the write journal plugin configuration section
  write-plugin = "cassandra-journal"

  read-profile = "cassandra-journal"
  refresh-interval = 100ms
  events-by-tag {
    refresh-interval = 100ms
    eventual-consistency-delay = 100ms
  }
}