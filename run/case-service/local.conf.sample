include "application"

projectionsDB = {
  profile = "slick.jdbc.PostgresProfile$"
  db {
    driver = org.postgresql.Driver
    url = "jdbc:postgresql://localhost:5432/postgres"
    // User name to connect, update and query
    user = "postgres"
    password = "postgres"
    // User name for migration of schema upon startup
    migrateUser = "postgres"
    migratePwd = "postgres"
    numThreads = 10
    connectionTimeout = 5000
    validationTimeout = 5000
  }
}

akka {
  loglevel = INFO
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 10s

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    serialize-messages = on

    serializers {
      command_serializer = "org.cafienne.akka.actor.serialization.CommandSerializer"
      response_serializer = "org.cafienne.akka.actor.serialization.ResponseSerializer"
      event_serializer = "org.cafienne.akka.actor.serialization.EventSerializer"
      // offset_serializer is used to serialize offset snapshots
      offset_serializer = "org.cafienne.infrastructure.eventstore.OffsetSerializer"
    }
    serialization-bindings {
      "org.cafienne.akka.actor.event.ModelEvent" = event_serializer
      "org.cafienne.akka.actor.command.ModelCommand" = command_serializer
      "org.cafienne.akka.actor.command.response.ModelResponse" = response_serializer
      // Current offsets are WrappedOffset objects
      "org.cafienne.infrastructure.eventstore.WrappedOffset" = offset_serializer
      // enable below to check if all events have been serialized without java.io.Serializable
      //"java.io.Serializable" = none
    }

    provider = "akka.cluster.ClusterActorRefProvider"
  }

  remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    seed-nodes = [
      "akka.tcp://ClusterSystem@127.0.0.1:2551",
      "akka.tcp://ClusterSystem@127.0.0.1:2552"]

    auto-down-unreachable-after = 10s
  }

  persistence {
    journal {
      # DO NOT USE LEVELDB FOR A MULTI NODE SETUP !!!
      # NOTE: Default journal is leveldb, as it comes out of the box without setup.
      # However, this cannot be used in production or in a multi-node setup.
      # In that case, the cassandra-journal has to be enabled.
      plugin = "akka.persistence.journal.leveldb"

      # Default configuration for leveldb storage of events.
      # Cassandra configuration is at the end of this file
      leveldb {
        store {
          # DO NOT USE 'native = off' IN PRODUCTION !!!
          native = off
          dir = "journal"
        }

        event-adapters {
          tagging = "org.cafienne.akka.actor.tagging.CaseTaggingEventAdapter"
        }

        event-adapter-bindings {
          "org.cafienne.akka.actor.event.ModelEvent" = tagging
        }
      }
    }
  }

  contrib.cluster {
    sharding {
      # The extension creates a top level actor with this name in top level user scope,
      # e.g. '/user/sharding'
      guardian-name = sharding
      # If the coordinator can't store state changes it will be stopped
      # and started again after this duration.
      coordinator-failure-backoff = 10 s
      # Start the coordinator singleton manager on members tagged with this role.
      # All members are used if undefined or empty.
      # ShardRegion actor is started in proxy only mode on nodes that are not tagged
      # with this role.

      #role = "case-domain"
      #role = "domain"

      # The ShardRegion retries registration and shard location requests to the
      # ShardCoordinator with this interval if it does not reply.
      retry-interval = 2 s
      # Maximum number of messages that are buffered by a ShardRegion actor.
      buffer-size = 100000
      # Timeout of the shard rebalancing process.
      handoff-timeout = 60 s
      # Rebalance check is performed periodically with this interval.
      rebalance-interval = 10 s
      # How often the coordinator saves persistent snapshots, which are
      # used to reduce recovery times
      snapshot-interval = 3600 s
      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        rebalance-threshold = 10
        # The number of ongoing rebalancing processes is limited to this number.
        max-simultaneous-rebalance = 3
      }
    }
  }
}

cafienne {
  # Platform has owners that are allowed to create/disable/enable tenants
  #  This property specifies the set of user-id's that are owners
  #  This array may not be empty.
  platform {
    owners = ["admin"]
  }

  api {
    bindhost = "localhost"
    bindport = 2027

    security {
      # configuration settings for OpenID Connect
      oidc {
        connect-url = "http://localhost:5556/dex/.well-known/openid-configuration"
        token-url = "http://127.0.0.1:5556/dex/token"
        key-url = "http://127.0.0.1:5556/dex/keys"
        authorization-url = "http://127.0.0.1:5556/dex/auth"
        issuer = "http://localhost:28080/dex"
      }
    }
  }

  # The case engine reads definitions as XML files from disk and/or the classpath.
  # The files are cached in-memory, based on their lastModified timestamp
  # (i.e., if you change a file on disk, the engine will reload it into the cache).
  # By default, the engine will read from the configured location. If the definitions file cannot be found
  # in this location, the engine will try to load it as a resource from the classpath, hence enabling to ship
  # fixed definitions in a jar file.
  definitions {
    provider = "org.cafienne.cmmn.repository.file.FileBasedDefinitionProvider"
    location = "./definitions"
    cache {
      size = 100
    }
  }
}
